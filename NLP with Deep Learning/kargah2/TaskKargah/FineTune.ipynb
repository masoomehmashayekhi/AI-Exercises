{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":14029575,"sourceType":"datasetVersion","datasetId":8933858}],"dockerImageVersionId":31193,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install -q transformers datasets scikit-learn torch shekar cleantext","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T18:23:15.731825Z","iopub.execute_input":"2025-12-06T18:23:15.732478Z","iopub.status.idle":"2025-12-06T18:23:19.357959Z","shell.execute_reply.started":"2025-12-06T18:23:15.732446Z","shell.execute_reply":"2025-12-06T18:23:19.357190Z"}},"outputs":[{"name":"stderr","text":"huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\nTo disable this warning, you can either:\n\t- Avoid using `tokenizers` before the fork if possible\n\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"import os\nos.environ[\"WANDB_DISABLED\"] = \"true\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:53.732252Z","iopub.execute_input":"2025-12-06T17:55:53.732613Z","iopub.status.idle":"2025-12-06T17:55:53.736499Z","shell.execute_reply.started":"2025-12-06T17:55:53.732584Z","shell.execute_reply":"2025-12-06T17:55:53.735959Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\ndf = pd.read_csv('/kaggle/input/dataset/final_generated_pairs.csv', header=None,names=['name1', 'name2','label'])\ndf = df.drop(df.index[0])\ndf = df.reset_index(drop=True)\ndf.head()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:53.737299Z","iopub.execute_input":"2025-12-06T17:55:53.737538Z","iopub.status.idle":"2025-12-06T17:55:54.193018Z","shell.execute_reply.started":"2025-12-06T17:55:53.737514Z","shell.execute_reply":"2025-12-06T17:55:54.192344Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"                name1               name2 label\n0  کالا پخش عصر ایلام   کالا پخش عصر ایلا     1\n1  کالا پخش عصر ایلام  کالا عصر ایلام پخش     1\n2  کالا پخش عصر ایلام  کالا پخش عصر ایلام     1\n3  کالا پخش عصر ایلام  کالا ایلام پخش عصر     1\n4  کالا پخش عصر ایلام  کالا عصر پخش ایلام     1","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>name1</th>\n      <th>name2</th>\n      <th>label</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>کالا پخش عصر ایلام</td>\n      <td>کالا پخش عصر ایلا</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>کالا پخش عصر ایلام</td>\n      <td>کالا عصر ایلام پخش</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>کالا پخش عصر ایلام</td>\n      <td>کالا پخش عصر ایلام</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>کالا پخش عصر ایلام</td>\n      <td>کالا ایلام پخش عصر</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>کالا پخش عصر ایلام</td>\n      <td>کالا عصر پخش ایلام</td>\n      <td>1</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"PERSIAN_STOPWORDS = {\n    \"شرکت\",\"موسسه\",\"گروه\",\"صنعت\",\"صنایع\",\"توسعه\",\"مهندسی\",\"فناوری\",\"نوین\",\n    \"تک\",\"ارتباط\",\"مبین\",\"پیشرفته\",\"گسترش\",\"مرکز\",\"هولدینگ\",\n    \"مدرن\",\"نو\",\"جدید\",\"پژوهش\",\"کاربردی\",\"راهکار\",\"راه\",\"راه‌حل\",\n    \"اندیشه\",\"سامانه\",\"خدمات\",\"تجارت\",\"تجاری\",\"بازرگانی\",\"کو\",\"ایران\",\n   \"و\", \"در\", \"با\", \"از\",\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:54.193672Z","iopub.execute_input":"2025-12-06T17:55:54.194339Z","iopub.status.idle":"2025-12-06T17:55:54.371291Z","shell.execute_reply.started":"2025-12-06T17:55:54.194320Z","shell.execute_reply":"2025-12-06T17:55:54.370523Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"from shekar import Normalizer, Stemmer, WordTokenizer, Lemmatizer\nfrom cleantext import clean\nimport re\n\nnormalizer = Normalizer()\nlemmatizer = Lemmatizer()\ntokenizer = WordTokenizer()\n\ndef preprocess_and_stem(text):\n    text = normalizer.normalize(text)\n    text = re.sub(r\"[^\\w\\s]\", \" \", text)\n    text = text.replace(\"\\u200c\", \"\")\n    text = str(clean(text,\n                           clean_all= False  ,\n                           extra_spaces=True ,\n                           numbers=True ,\n                           punct=True\n                           ))\n\n    tokens = list(tokenizer(text))\n\n    stems =  [lemmatizer(t) for t in tokens if t not in PERSIAN_STOPWORDS]\n    return \" \".join(stems)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T18:23:23.965640Z","iopub.execute_input":"2025-12-06T18:23:23.966303Z","iopub.status.idle":"2025-12-06T18:23:24.396259Z","shell.execute_reply.started":"2025-12-06T18:23:23.966274Z","shell.execute_reply":"2025-12-06T18:23:24.395497Z"}},"outputs":[],"execution_count":26},{"cell_type":"code","source":"df[\"label\"] = df[\"label\"].map({\n    \"1\": 1,\n    \"0\": 0\n}).astype(int)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:56.501321Z","iopub.execute_input":"2025-12-06T17:55:56.501932Z","iopub.status.idle":"2025-12-06T17:55:56.744647Z","shell.execute_reply.started":"2025-12-06T17:55:56.501911Z","shell.execute_reply":"2025-12-06T17:55:56.743733Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n \ntrain_df, temp_df = train_test_split(df, test_size=0.3, random_state=42, stratify=df['label'])\nval_df, test_df = train_test_split(temp_df, test_size=0.5, random_state=42, stratify=temp_df['label'])\n\nprint(f\"Train size: {len(train_df)}, Validation size: {len(val_df)}, Test size: {len(test_df)}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:56.745530Z","iopub.execute_input":"2025-12-06T17:55:56.745797Z","iopub.status.idle":"2025-12-06T17:55:57.694532Z","shell.execute_reply.started":"2025-12-06T17:55:56.745770Z","shell.execute_reply":"2025-12-06T17:55:57.693878Z"}},"outputs":[{"name":"stdout","text":"Train size: 114437, Validation size: 24522, Test size: 24523\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"from datasets import Dataset\n\ntrain_dataset = Dataset.from_pandas(train_df)\nval_dataset = Dataset.from_pandas(val_df)\ntest_dataset = Dataset.from_pandas(test_df)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:57.695301Z","iopub.execute_input":"2025-12-06T17:55:57.695603Z","iopub.status.idle":"2025-12-06T17:55:58.368210Z","shell.execute_reply.started":"2025-12-06T17:55:57.695557Z","shell.execute_reply":"2025-12-06T17:55:58.367596Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"from transformers import AutoTokenizer\n\nmodel_name = \"PartAI/TookaBERT-Base\"   \ntokenizer = AutoTokenizer.from_pretrained(model_name)\n \ndef tokenize_function(example):\n    return tokenizer(example['name1'], example['name2'], truncation=True, max_length=128)\n\ntrain_dataset = train_dataset.map(tokenize_function, batched=True)\nval_dataset = val_dataset.map(tokenize_function, batched=True)\ntest_dataset = test_dataset.map(tokenize_function, batched=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:55:58.368932Z","iopub.execute_input":"2025-12-06T17:55:58.369135Z","iopub.status.idle":"2025-12-06T17:56:08.498452Z","shell.execute_reply.started":"2025-12-06T17:55:58.369114Z","shell.execute_reply":"2025-12-06T17:56:08.497871Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/463 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8977447539454778968a889789e23ccf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc57784226d14bc3b25787e363315b3d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/145 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a05f957315a444acae16231de3a41e94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/114437 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"361cf1a50aea42d3a5e16c9e119b920e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24522 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5903ccff72d34c5c8985f87d6cea4c05"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/24523 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5807d1b67ec34a53be9e85614876e678"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"import torch\nfrom transformers import DataCollatorWithPadding\n\ntrain_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\nval_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\ntest_dataset.set_format(type='torch', columns=['input_ids', 'attention_mask', 'label'])\n\ndata_collator = DataCollatorWithPadding(tokenizer=tokenizer)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:56:08.499165Z","iopub.execute_input":"2025-12-06T17:56:08.499396Z","iopub.status.idle":"2025-12-06T17:56:08.505260Z","shell.execute_reply.started":"2025-12-06T17:56:08.499372Z","shell.execute_reply":"2025-12-06T17:56:08.504688Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from transformers import AutoModelForSequenceClassification\n\nnum_labels = 2  \nmodel = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:56:08.505899Z","iopub.execute_input":"2025-12-06T17:56:08.506500Z","iopub.status.idle":"2025-12-06T17:56:11.497163Z","shell.execute_reply.started":"2025-12-06T17:56:08.506482Z","shell.execute_reply":"2025-12-06T17:56:11.496385Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/730 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0db5e4b10a9442448e5d81556857b4e7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/492M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"414b0ae14dc24b56b17f23b56cf60071"}},"metadata":{}},{"name":"stderr","text":"Some weights of BertForSequenceClassification were not initialized from the model checkpoint at PartAI/TookaBERT-Base and are newly initialized: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'classifier.bias', 'classifier.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"}],"execution_count":14},{"cell_type":"code","source":"from transformers import Trainer, TrainingArguments\nfrom sklearn.metrics import accuracy_score, precision_recall_fscore_support\n\ntraining_args = TrainingArguments(\n    output_dir=\"./tookabert-finetuned\",   \n    save_steps=3000, \n    learning_rate=2e-5,\n    per_device_train_batch_size=16,\n    per_device_eval_batch_size=16,\n    num_train_epochs=2,\n    weight_decay=0.01,\n    logging_dir=None,\n    metric_for_best_model=\"accuracy\",\n    fp16=True\n)\n\ndef compute_metrics(eval_pred):\n    logits, labels = eval_pred\n    predictions = logits.argmax(axis=-1)\n    precision, recall, f1, _ = precision_recall_fscore_support(labels, predictions, average='binary')\n    acc = accuracy_score(labels, predictions)\n    return {\"accuracy\": acc, \"f1\": f1, \"precision\": precision, \"recall\": recall}\n\ntrainer = Trainer(\n    model=model,\n    args=training_args,\n    train_dataset=train_dataset,\n    eval_dataset=val_dataset,\n    tokenizer=tokenizer,\n    data_collator=data_collator,\n    compute_metrics=compute_metrics\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:56:11.498025Z","iopub.execute_input":"2025-12-06T17:56:11.498250Z","iopub.status.idle":"2025-12-06T17:56:12.021703Z","shell.execute_reply.started":"2025-12-06T17:56:11.498226Z","shell.execute_reply":"2025-12-06T17:56:12.020956Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n/tmp/ipykernel_47/2906843356.py:24: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n  trainer = Trainer(\n","output_type":"stream"}],"execution_count":15},{"cell_type":"code","source":"print(\"PyTorch version:\", torch.__version__)\nprint(\"CUDA available:\", torch.cuda.is_available())\nif torch.cuda.is_available():\n    print(\"Device name:\", torch.cuda.get_device_name(0))\n    print(\"Device count:\", torch.cuda.device_count())\nelse:\n    print(\"Running on CPU\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:56:12.022519Z","iopub.execute_input":"2025-12-06T17:56:12.022731Z","iopub.status.idle":"2025-12-06T17:56:12.027439Z","shell.execute_reply.started":"2025-12-06T17:56:12.022715Z","shell.execute_reply":"2025-12-06T17:56:12.026678Z"}},"outputs":[{"name":"stdout","text":"PyTorch version: 2.6.0+cu124\nCUDA available: True\nDevice name: Tesla T4\nDevice count: 2\n","output_type":"stream"}],"execution_count":16},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T17:56:12.029609Z","iopub.execute_input":"2025-12-06T17:56:12.029968Z","iopub.status.idle":"2025-12-06T18:19:43.832587Z","shell.execute_reply.started":"2025-12-06T17:56:12.029951Z","shell.execute_reply":"2025-12-06T18:19:43.831982Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='7154' max='7154' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [7154/7154 23:29, Epoch 2/2]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>500</td>\n      <td>0.039700</td>\n    </tr>\n    <tr>\n      <td>1000</td>\n      <td>0.004400</td>\n    </tr>\n    <tr>\n      <td>1500</td>\n      <td>0.004200</td>\n    </tr>\n    <tr>\n      <td>2000</td>\n      <td>0.010200</td>\n    </tr>\n    <tr>\n      <td>2500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>3000</td>\n      <td>0.003900</td>\n    </tr>\n    <tr>\n      <td>3500</td>\n      <td>0.003100</td>\n    </tr>\n    <tr>\n      <td>4000</td>\n      <td>0.001900</td>\n    </tr>\n    <tr>\n      <td>4500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>5000</td>\n      <td>0.002500</td>\n    </tr>\n    <tr>\n      <td>5500</td>\n      <td>0.003300</td>\n    </tr>\n    <tr>\n      <td>6000</td>\n      <td>0.002200</td>\n    </tr>\n    <tr>\n      <td>6500</td>\n      <td>0.000000</td>\n    </tr>\n    <tr>\n      <td>7000</td>\n      <td>0.000200</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"execution_count":17,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=7154, training_loss=0.005350069574532327, metrics={'train_runtime': 1411.3626, 'train_samples_per_second': 162.165, 'train_steps_per_second': 5.069, 'total_flos': 2651845451984220.0, 'train_loss': 0.005350069574532327, 'epoch': 2.0})"},"metadata":{}}],"execution_count":17},{"cell_type":"code","source":"results = trainer.evaluate(test_dataset)\nprint(results)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T18:19:43.833422Z","iopub.execute_input":"2025-12-06T18:19:43.833658Z","iopub.status.idle":"2025-12-06T18:20:33.080270Z","shell.execute_reply.started":"2025-12-06T18:19:43.833642Z","shell.execute_reply":"2025-12-06T18:20:33.079478Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/parallel/_functions.py:70: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='767' max='767' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [767/767 00:49]\n    </div>\n    "},"metadata":{}},{"name":"stdout","text":"{'eval_loss': 0.00036790024023503065, 'eval_accuracy': 0.9999592219548995, 'eval_f1': 0.9999474927802573, 'eval_precision': 1.0, 'eval_recall': 0.9998949910742413, 'eval_runtime': 49.2366, 'eval_samples_per_second': 498.064, 'eval_steps_per_second': 15.578, 'epoch': 2.0}\n","output_type":"stream"}],"execution_count":18},{"cell_type":"code","source":"model.save_pretrained(\"./tookabert-finetuned\")\ntokenizer.save_pretrained(\"./tookabert-finetuned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T18:20:33.081187Z","iopub.execute_input":"2025-12-06T18:20:33.081401Z","iopub.status.idle":"2025-12-06T18:20:33.928789Z","shell.execute_reply.started":"2025-12-06T18:20:33.081383Z","shell.execute_reply":"2025-12-06T18:20:33.928210Z"}},"outputs":[{"execution_count":19,"output_type":"execute_result","data":{"text/plain":"('./tookabert-finetuned/tokenizer_config.json',\n './tookabert-finetuned/special_tokens_map.json',\n './tookabert-finetuned/tokenizer.json')"},"metadata":{}}],"execution_count":19},{"cell_type":"code","source":"def check_name_validity(text1:str, text2 :str):\n    inputs = tokenizer(text1, text2, return_tensors=\"pt\")\n\n    with torch.no_grad():\n        outputs = model(**inputs)\n\n    prob = outputs.logits.softmax(dim=1)[0][1].item()\n    return prob","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-12-06T18:22:12.672543Z","iopub.execute_input":"2025-12-06T18:22:12.673165Z","iopub.status.idle":"2025-12-06T18:22:12.677430Z","shell.execute_reply.started":"2025-12-06T18:22:12.673141Z","shell.execute_reply":"2025-12-06T18:22:12.676881Z"}},"outputs":[],"execution_count":21}]}