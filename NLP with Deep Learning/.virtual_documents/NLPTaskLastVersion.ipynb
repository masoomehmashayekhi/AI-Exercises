


get_ipython().getoutput("pip install -q pandas scikit-learn matplotlib")
get_ipython().getoutput("pip install -q sentence-transformers")
get_ipython().getoutput("pip install -q umap-learn")
get_ipython().getoutput("pip install -q huggingface_hub")


get_ipython().getoutput("pip install "hazm[full]" --no-deps && pip install numpy==1.26.4 fasttext-wheel flashtext gensim==4.3.3 nltk python-crfsuite")


import pandas as pd
import numpy as np
import matplotlib.pyplot as plt


# Login using e.g. `huggingface-cli login` to access this dataset
splits = {'train': 'train.csv', 'validation': 'validation.csv', 'test': 'test.csv'}
df_train = pd.read_csv("hf://datasets/ParsiAI/snappfood-sentiment-analysis/" + splits["train"])
df_validation = pd.read_csv("hf://datasets/ParsiAI/snappfood-sentiment-analysis/" + splits["validation"])
df_test = pd.read_csv("hf://datasets/ParsiAI/snappfood-sentiment-analysis/" + splits["test"])


print("TrainSet Shape:" ,df_train.shape)
print("ValidationSet Shape:" ,df_validation.shape)
print("TestSet Shape:" ,df_test.shape)


df_train.head()


df_train['label_id'].value_counts().sort_index()


print(df_train['comment'].isna().any())
print(df_train['label_id'].isna().any())


from hazm import *

normalizer = Normalizer()
df_train['comment_clean'] = df_train['comment'].astype(str).apply(normalizer.normalize)
df_validation['comment_clean'] = df_validation['comment'].astype(str).apply(normalizer.normalize)
df_test['comment_clean'] = df_test['comment'].astype(str).apply(normalizer.normalize)
df_train.head()


from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, precision_recall_fscore_support, classification_report, confusion_matrix
import numpy as np
import matplotlib.pyplot as plt

tfidf_vectorixer= TfidfVectorizer(ngram_range=(1,2),
                                  min_df=5,
                                  max_df=0.9)

y_train=df_train['label_id']
y_test =df_test['label_id']
y_validation =df_validation['label_id']

X_train = tfidf_vectorixer.fit_transform(df_train['comment_clean'])
X_validation = tfidf_vectorixer.transform(df_validation['comment_clean'])
X_test = tfidf_vectorixer.transform(df_test['comment_clean'])
X_train.shape



from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report

def accuracy(str,cl):
  y_pred = cl.predict(X_test)
  accuracy = accuracy_score(y_test, y_pred)

  # Precision
  precision = precision_score(y_test, y_pred, average='weighted')

  # Recall
  recall = recall_score(y_test, y_pred, average='weighted')

  # F1-score
  f1 = f1_score(y_test, y_pred, average='weighted')

  # Confusion matrix
  cm = confusion_matrix(y_test, y_pred)

  # گزارش کامل
  report = classification_report(y_test, y_pred)

  print(f"--------{str} scores-----")
  print(f"-------------------------------------")
  print("Accuracy :", accuracy)
  print("Precision:", precision)
  print("Recall   :", recall)
  print("F1-score :", f1)
  print("\nConfusion Matrix:\n", cm)
  print("\nClassification Report:\n", report)


from sklearn.linear_model import LogisticRegression


cl= LogisticRegression(max_iter=3000)
cl.fit(X_train,y_train)
accuracy("LogisticRegression",cl)


from sklearn.naive_bayes import MultinomialNB


svm= MultinomialNB()
svm.fit(X_train,y_train)
accuracy("MultinomialNB",svm)


from sklearn.ensemble import RandomForestClassifier


rf= RandomForestClassifier(n_estimators=300)
rf.fit(X_train,y_train)
accuracy("RandomForestClassifier",rf)


import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional
from tensorflow.keras.utils import to_categorical
from tensorflow.keras.callbacks import EarlyStopping

print(tf.config.list_physical_devices('GPU'))

tokenizer = Tokenizer(num_words=5000, oov_token="<OOV>")
tokenizer.fit_on_texts(df_train['comment_clean'])

sequences = tokenizer.texts_to_sequences(df_train['comment_clean'])
X_train = pad_sequences(sequences, maxlen=50, padding='post', truncating='post')

y_train = to_categorical(df_train['label_id'])


sequences_valid = tokenizer.texts_to_sequences(df_validation['comment_clean'])
X_valid = pad_sequences(sequences_valid, maxlen=50, padding='post', truncating='post')

y_validation = to_categorical(df_validation['label_id'])


sequences_test = tokenizer.texts_to_sequences(df_test['comment_clean'])
X_test = pad_sequences(sequences_test, maxlen=50, padding='post', truncating='post')

y_test = to_categorical(df_test['label_id'])

model = Sequential([
    Embedding(5000, 128, input_length=50),
    Bidirectional(LSTM(64, return_sequences=True)),
    Dropout(0.5),
    Bidirectional(LSTM(32)),
    Dropout(0.5),
    Dense(32, activation='relu', kernel_regularizer=tf.keras.regularizers.l2(0.001)),
    Dense(2, activation='softmax')
])

es = EarlyStopping(
    monitor='val_loss',
    patience=2,
    restore_best_weights=True
)

model.compile(optimizer='adam',
    loss='binary_crossentropy',
    metrics=['accuracy'])

model.build(input_shape=(None, 50))
model.summary()




history = model.fit(X_train, y_train, epochs=10, batch_size=16,validation_data=(X_valid, y_validation), verbose=1,callbacks=[es])

loss, acc = model.evaluate(X_test, y_test)


y_pred_rnn = model.predict(X_test)
y_test_prob = np.argmax(y_test, axis=1)


y_pred_rnn_prob = np.argmax(y_pred_rnn, axis=1)

acc_rnn = accuracy_score(y_test_prob, y_pred_rnn_prob)
p_rnn, r_rnn, f1_rnn, _ = precision_recall_fscore_support(y_test_prob, y_pred_rnn_prob, average='weighted')


cm_rnn = confusion_matrix(y_test_prob, y_pred_rnn_prob)


print(f"--------RNN scores-----")
print(f"---------------------")
print("Accuracy :", acc_rnn)
print("Precision:", p_rnn)
print("Recall   :", r_rnn)
print("F1-score :", f1_rnn)
print("\nConfusion Matrix:\n", cm_rnn)


from datasets import Dataset

label2id = {"negative": 0, "positive": 1}
id2label = {0: "negative", 1: "positive"}

dataset = Dataset.from_pandas(df_train)
datatest = Dataset.from_pandas(df_test)

dataset


from transformers import (
    AutoTokenizer,
    AutoModelForSequenceClassification,
    TrainingArguments,
    Trainer,
    DataCollatorWithPadding
)


model_name = "HooshvareLab/bert-fa-zwnj-base"

tokenizer = AutoTokenizer.from_pretrained(model_name)
model = AutoModelForSequenceClassification.from_pretrained(
    model_name,
    num_labels=2,
    id2label=id2label,
    label2id=label2id
)


def preprocess_function(examples):
    return tokenizer(examples["comment_clean"], truncation=True, padding=True, max_length=128)

tokenized_dataset_train = dataset.map(preprocess_function, batched=True)
tokenized_dataset_test = datatest.map(preprocess_function, batched=True)



tokenized_train = tokenized_dataset_train.map(lambda x: {"labels": int(x["label_id"])})
tokenized_test  = tokenized_dataset_test.map(lambda x: {"labels": int(x["label_id"])})


cols_to_remove = ['comment', 'comment_clean', 'label', 'label_id']

tokenized_train = tokenized_train.remove_columns([c for c in cols_to_remove if c in tokenized_train.column_names])
tokenized_test  = tokenized_test.remove_columns([c for c in cols_to_remove if c in tokenized_test.column_names])



tokenized_train.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
tokenized_test.set_format(type='torch', columns=['input_ids', 'attention_mask', 'labels'])
tokenized_train


tokenized_train.column_names
tokenized_train[0]


from sklearn.metrics import accuracy_score, precision_recall_fscore_support

def compute_metrics(eval_pred):
    logits, labels = eval_pred
    preds = np.argmax(logits, axis=-1)
    acc = accuracy_score(labels, preds)
    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average='weighted')
    return {"accuracy": acc, "f1": f1, "precision": p, "recall": r}


from transformers import DataCollatorWithPadding, EarlyStoppingCallback
data_collator = DataCollatorWithPadding(tokenizer=tokenizer)

training_args = TrainingArguments(
    output_dir="./sentiment_farsi_model",
    num_train_epochs=5,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=32,
    warmup_steps=100,
    weight_decay=0.01,
    logging_dir='./logs',
    logging_steps=10,
    eval_strategy="epoch",
    save_strategy="epoch",
    load_best_model_at_end=True,
    metric_for_best_model="f1",
    greater_is_better=True,
    fp16=True,
    dataloader_num_workers=4,
    report_to="none",
    seed=42
)

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=tokenized_train,
    eval_dataset=tokenized_test,
    tokenizer=tokenizer,
    data_collator=data_collator,
    compute_metrics=compute_metrics,
    callbacks=[EarlyStoppingCallback(early_stopping_patience=2)]
)


import torch
print("GPU available:", torch.cuda.is_available())
print("Device:", torch.cuda.get_device_name(0) if torch.cuda.is_available() else "No GPU")


trainer.train()


trainer.evaluate()
