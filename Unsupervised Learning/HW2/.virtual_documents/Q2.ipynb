import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import scipy.optimize as so
import math
from sklearn import linear_model
from IPython.display import display






def laplaceNegLogLikelihood(mu,lam,y):
    # Compute and return nll (negative log likelihood)
    nll = -np.sum(-np.log(2*lam) - abs(y - mu) / lam)
    return nll

#Test your function with these values
mu=0
lam=1
y=np.array([2,1,0,0])

# ****** write your code here ******
print(laplaceNegLogLikelihood(mu,lam,y))





def laplaceRegNegLogLikelihood(b, X, y):
    # Compute mu
    mu = np.exp(X@b)
    #print(mu)
    # Compute negative log likelihood (let lam = 1)
    lam = 1
    nll = laplaceNegLogLikelihood(mu,lam,y)
    return nll

#Test your function with these values
b=np.array([1,2])
X=np.array([[1,1,1],[0,2,1]]).T
y=np.array([0,2,10])
#print(y)

# ****** your code here ******
print('NLL =',laplaceRegNegLogLikelihood(b, X, y))





def modelPredict(b,X):
    #Compute yhat
    yhat = np.exp(X@b)
    return yhat

#Test your function with these values
b=np.array([1,2])
X=np.array([[1,1,1],[0,2,1]]).T
# print(X)

# ****** your code here ******
print('yhat =',modelPredict(b,X))





def modelFit(X,y):
    b_init = np.zeros(X.shape[1])
    # Start from b_init. Use so.minimize to get a prediction for b which maximizes Laplace Log Likelihood 
    RES = so.minimize(laplaceRegNegLogLikelihood, b_init, args=(X,y), method="Powell", tol = 1e-8)
    return RES.x

#Test your function with these values
X = np.array([[1,1,1],[0,2,1]]).T
y = np.array([0,2,10])

# ****** your code here ******
b0,b1= modelFit(X,y)
print("b0: ",b0," | b1: ",b1)





# Load in the data
df = pd.read_csv("Q2.csv")
print(df.head)
# Plot a scatterplot with labels
# ****** your code here ******
plt.scatter(df["x"],df["y"])
plt.title("x vs y Dataset")
plt.xlabel("x = Geyser eruption times")
plt.ylabel("y = Waiting times (minutes)")





from pylab import rcParams
rcParams['figure.figsize'] = 15, 10
#First, construct the matrix X (with a column of ones and a column of x values)
x = df.x.values
X = np.c_[np.ones(x.size),x]
#Next, find b by calling modelFit. modelFit should optimize the laplaceRegressionNegLogLikelihood
y = df.y.values
b = modelFit(X,y)

# For our prediction we need to create new x data (use linspace) and put it in matrix X
x_new = np.linspace(0,6, 200)
X_new = np.c_[np.ones(x_new.size), x_new]

# Use modelPredict to get y_pred [ /2 marks]
y_pred = modelPredict(b, X_new) #yhat
#print(y_pred)

#Plot the data points and predictions on the same plot; color the predictions red
# ****** your code here ******
plt.scatter(df['x'],df['y'])
plt.plot(X_new[:,1],y_pred, color='red')
plt.title("x vs y dataset with Laplace regression prediction line")
plt.xlabel("x = Geyser eruption times")
plt.ylabel("y = Waiting times (minutes)")
#plt.plot(X_new,y_pred,c='r')





from pylab import rcParams
rcParams['figure.figsize'] = 15, 10

#First, construct the matrix X (with a column of ones and a column of x values)
x = df.x.values
X = np.c_[np.ones(x.size),x]

# Get y values, then use sklearn's linear_model.LinearRegression().fit() to create an L2 fit (using X and y)
y = df.y.values
L2_fit = linear_model.LinearRegression().fit(X,y)

# For our prediction we need to create new x data (use linspace) and put this in matrix X_new
x_new = np.linspace(0,6, 200)
X_new = np.c_[np.ones(x_new.size), x_new]

# Use sklearn's "predict" method to get your prediction, given an input matrix X_new y_pred_L2
y_pred_L2 = L2_fit.predict(X_new)
#print(y_pred_L2)
# Plot the data and linear regression on the same figure. Label axes
# ****** your code here ******
plt.scatter(df['x'],df['y'])
plt.plot(X_new[:,1],y_pred_L2, color='red')
plt.title("x vs y dataset with linear regression (L2) prediction line")
plt.xlabel("x = Geyser eruption times")
plt.ylabel("y = Waiting times (minutes)")





print("The dataset has a wide range of points scattered though the graph. A Linear regression model is not able to obtain a descent predicion as it does not take into account some of the outliers (not robust to outliers). Instead Linear regression model is underfitted. In contrast, the Laplacian regression is a much better fit as it subtracts each point's y value from the y value of the line at that x value (Laplace distribution formula). This is is the residual for that point.")
