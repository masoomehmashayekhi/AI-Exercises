import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns
import math
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import MinMaxScaler
from sklearn.linear_model import LogisticRegression
from sklearn.naive_bayes import GaussianNB, MultinomialNB
from sklearn.metrics import confusion_matrix

raw_data = pd.read_csv("./anti-malware .csv", skiprows=1, names=range(14))

#print(raw_data.isna().sum())
y = raw_data.pop(13)


min_max_scaler = MinMaxScaler()
X = min_max_scaler.fit_transform(raw_data)

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

lr_cls = LogisticRegression(multi_class="multinomial")
lr_cls.fit(X_train, y_train)

y_pred1 = lr_cls.predict(X_test)

cm = confusion_matrix(y_test, y_pred1)
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".0f")
plt.title("LogisticRegression", fontsize=16)
plt.show()

gnb = GaussianNB()
gnb.fit(X_train, y_train)

y_pred2 = gnb.predict(X_test)

cm = confusion_matrix(y_test, y_pred2)
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".0f")
plt.title("GaussianNB", fontsize=16)
plt.show()

mnb = MultinomialNB()
mnb.fit(X_train, y_train)

y_pred3 = mnb.predict(X_test)

cm = confusion_matrix(y_test, y_pred3)
sns.heatmap(cm, annot=True, cmap="Blues", fmt=".0f")
plt.title("MultinomialNB", fontsize=16)
plt.show()

#به نظر میرسه که ماتریس رگرسیون منطقی و بیز چند جمله ای شبیه به هم است
# در مقایسه با بیز گاوسی و رگرسیون منطقی، رگرسیون منطقی بهتر عمل کرده است زیرا ماتریس به ماتریس قطری نزدیکتر است



